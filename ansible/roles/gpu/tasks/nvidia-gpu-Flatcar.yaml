---
- name: create /opt/lib directory
  file:
    path: "/opt/lib/"
    state: directory

- name: create /opt/var/log directory
  file:
    path: "/opt/var/log/"
    state: directory

# Check if the kernel modules exist.
- name: check if the nvidia kernel modules exist
  changed_when: false
  stat:
    path: /opt/nvidia/{{ nvidia_driver_version }}
  register: kmodules

- name: nvidia kernel modules build
  when:
    - not kmodules.stat.exists
  block:
    # The following modules are needed for the build of kernel modules on flatcar.
    # TODO(tillt): Why does ansible modprobe fail here, it should not.
    - name: load loop modules
      command: modprobe -d / -a loop i2c_core ipmi_msghandler

    - name: create build directory
      ansible.builtin.tempfile:
        state: directory
        suffix: driver
      register: driver_dir

    # The kernel modules as well as a large part of the libraries and binaries like
    # `nvidia-smi` are built within this container. All of these strictly belong together
    # in terms of versioning - especially libnvidia-ml has to exactly match the kernel
    # modules version.
    # Note: We do not have to pull the exact matching image for the kernel version as the
    # image contained process pulls the correct / current kernel headers. We should be fine
    # staying on this very version until we are not.
    - name: copy nvidia-driver build Dockerfile
      copy:
        src: "./Dockerfile.nvidia-driver"
        dest: "{{ driver_dir.path }}/Dockerfile.nvidia-driver"
        mode: 0644

    - name: copy nvidia-driver build script
      copy:
        src: "./nvidia-driver"
        dest: "{{ driver_dir.path }}/nvidia-driver"
        mode: 0755

    - name: copy key
      copy:
        src: "./empty"
        dest: "{{ driver_dir.path }}/empty"
        mode: 0644

    - name: build nvidia-driver build image
      command: docker build -f {{ driver_dir.path }}/Dockerfile.nvidia-driver --build-arg DRIVER_VERSION={{ nvidia_driver_version }} --tag nvidia-driver {{ driver_dir.path }}

    # - name: pull driver build image
    #   command: "docker pull driver:{{ nvidia_driver_version }}-5.10.77-flatcar"

    # The build runs in background so we can monitor its console output.
    # Note: This has be to run privileged as the driver module is loaded (briefly).
    - name: run the build
      command: docker run -d --rm --privileged --pid=host -v /run/nvidia:/run/nvidia:shared -v /tmp/nvidia:/var/log -v /usr/lib64/modules:/usr/lib64/modules -v /opt:/opt --name nvidia-driver nvidia-driver:latest init

    # The build commonly takes about 10 minutes in total, until the console shows
    # "Done, now waiting for signal".
    # There are at least two well known scenarios in which this will just timeout:
    # - the instance is failing the driver load cause it is not offering a GPU
    # - the root volume size is too small as the build can swallow a whopping extra 10GB
    - name: wait for build being done
      command: "docker logs --tail 1 nvidia-driver"
      register: build_log
      retries: 150
      delay: 10
      until: "'Done, now waiting for signal' in build_log.stdout"

    # TODO(tillt): Consider a proper list of libs to grab - for that we would need a proper dependency tree.
    - name: tar nvidia-libs /usr/lib/x86_64-linux-gnu
      command: docker exec -i nvidia-driver "/bin/sh" -c "tar czf /opt/lib/x86_64-linux-gnu.tar.gz /usr/lib/x86_64-linux-gnu/libnvidia*"

    - name: tar nvidia-bins /usr/bin/nvidia*
      command: docker exec -i nvidia-driver "/bin/sh" -c "tar czf /opt/bin/nvidia-bins.tar.gz /usr/bin/nvidia*"

    # We got all the artefacts we needed, we can safely tell the build container to gracefully stop.
    - name: send signal to terminate build container
      command: "docker kill -s SIGHUP nvidia-driver"

    - name: extract libs into /opt/lib/x86_64-linux-gnu
      command: "tar xzf /opt/lib/x86_64-linux-gnu.tar.gz --directory /opt/lib --strip-components 2"

    - name: remove x86_64-linux-gnu tarball
      command: "rm /opt/lib/x86_64-linux-gnu.tar.gz"

    - name: extract bins into /opt/bin
      command: "tar xzf /opt/bin/nvidia-bins.tar.gz --directory /opt/bin --strip-components 2"

    - name: remove nvidia-bins tarball
      command: "rm /opt/bin/nvidia-bins.tar.gz"

- name: run ldconfig
  changed_when: false
  command: ldconfig /opt/lib/x86_64-linux-gnu

# We load the kernel modules now - but only for validation purposes.
# Note, we can't use ansible modprobe here as we need to provide a non-
# standard location.
- name: load nvidia modules for validation
  changed_when: false
  command: modprobe -d /opt/nvidia/{{ nvidia_driver_version }}/ -a nvidia nvidia-uvm nvidia-modeset

- name: create systemd unit file for nvidia-driver service
  template:
    dest: /etc/systemd/system/nvidia-driver.service
    src: etc/systemd/system/nvidia-driver.service
    mode: 0600

- name: create systemd unit file for nvidia-persistenced service
  template:
    dest: /etc/systemd/system/nvidia-persistenced.service
    src: etc/systemd/system/nvidia-persistenced.service
    mode: 0600

# Check if Container Toolkit exists.
- name: check if the nvidia-container-toolkit exists
  changed_when: false
  stat:
    path: /opt/bin/nvidia-container-runtime-hook
  register: container_toolkit

- name: nvidia-container-toolkit build
  when:
    - not container_toolkit.stat.exists
  block:
    # Build Container Toolkit
    - name: create temporary build directory
      ansible.builtin.tempfile:
        state: directory
        suffix: build
      register: toolkit_dir

    - name: copy nvidia-container-toolkit build Dockerfile
      copy:
        src: "./Dockerfile.containertools"
        dest: "{{ toolkit_dir.path }}/Dockerfile.containertools"

    - name: build nvidia-container-toolkit build image
      command: docker build -f {{ toolkit_dir.path }}/Dockerfile.containertools --build-arg GOLANG_VERSION=1.16.5 --tag container-toolkit-build {{ toolkit_dir.path }}

    - name: clone nvidia-container-toolkit sources
      command: "git clone --depth 1 --branch v{{ nvidia_container_toolkit_version }} https://gitlab.com/nvidia/container-toolkit/container-toolkit.git {{ toolkit_dir.path }}/container-toolkit"

    - name: build nvidia-container-toolkit
      command: docker run -i -v {{ toolkit_dir.path }}/container-toolkit:/go/src/nvidia-container-toolkit -w /go/src/nvidia-container-toolkit container-toolkit-build:latest "/bin/sh" -c "make binary"

    - name: copy nvidia-container-toolkit
      command: cp {{ toolkit_dir.path }}/container-toolkit/nvidia-container-toolkit /opt/bin

    - name: link nvidia-container-toolkit-hook
      file:
        src: "/opt/bin/nvidia-container-toolkit"
        dest: "/opt/bin/nvidia-container-runtime-hook"
        state: link

# Check if Nvidia container runtime exists.
- name: check if the nvidia-container-runtime exists
  changed_when: false
  stat:
    path: /opt/usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
  register: container_runtime

# Build Container Runtime
- name: nvidia-container-runtime build
  when:
    - not container_runtime.stat.exists
  block:
    # Build Container Runtime
    - name: create temporary build directory
      ansible.builtin.tempfile:
        state: directory
        suffix: build
      register: build_dir

    - name: clone nvidia-container-runtime sources
      command: "git clone --depth 1 --branch v{{ nvidia_container_runtime_version }} https://gitlab.com/nvidia/container-toolkit/container-runtime.git {{ build_dir.path }}/container-runtime"

    - name: pull golang image
      command: "docker pull golang:1.16"

    - name: build the nvidia-container-runtime
      command: docker run -i -v  {{ build_dir.path }}/container-runtime:/tmp/container-runtime -w /tmp/container-runtime golang:1.16 "/usr/bin/make" "build"

    # Install Container Runtime
    - name: copy nvidia-container-runtime
      command: cp {{ build_dir.path }}/container-runtime/nvidia-container-runtime /opt/bin

    - name: create nvidia-container-runtime configuration directory
      file:
        path: "/etc/nvidia-container-runtime/"
        state: directory

    - name: copy nvidia-container-runtime configuration
      template:
        src: "./etc/nvidia-container-runtime/config.toml"
        dest: "/etc/nvidia-container-runtime/config.toml"

    - name: create oci hooks directory
      file:
        path: "/opt/usr/libexec/oci/hooks.d/"
        state: directory

    - name: copy oci hook
      template:
        src: "./usr/libexec/oci/hooks.d/oci-nvidia-hook"
        dest: "/opt/usr/libexec/oci/hooks.d/oci-nvidia-hook"
        mode: 0755

    - name: create oci hooks configuration directory
      file:
        path: "/opt/usr/share/containers/oci/hooks.d/"
        state: directory

    - name: copy oci hook configuration
      template:
        src: "./usr/share/containers/oci/hooks.d/oci-nvidia-hook.json"
        dest: "/opt/usr/share/containers/oci/hooks.d/oci-nvidia-hook.json"

# Check if libnvidia-container exists.
- name: check if the libnvidia-container exists
  changed_when: false
  stat:
    path: /opt/lib/x86_64-linux-gnu/libnvidia-container.so
  register: libnvidia_container

# Build Container Runtime
- name: libnvidia-container build
  when:
    - not libnvidia_container.stat.exists
  block:
    # Build libnvidia-container including nvidia-container-cli - a dependency of
    # nvidia-container-runtime.
    - name: create temporary build directory
      ansible.builtin.tempfile:
        state: directory
        suffix: build
      register: libnvidia_dir

    - name: clone libnvidia-container sources
      command: "git clone --depth 1 --branch v{{ libnvidia_container_version }} https://gitlab.com/nvidia/container-toolkit/libnvidia-container.git {{ libnvidia_dir.path }}/libnvidia-container"

    - name: copy libnvidia build Dockerfile
      copy:
        src: "./Dockerfile.libnvidia"
        dest: "/tmp/Dockerfile.libnvidia"

    - name: build the libnvidia-container build image
      command: docker build -f /tmp/Dockerfile.libnvidia --build-arg BUILD_LOCATION={{ libnvidia_dir.path }}/ --tag libnvidia-build ./

    # FIXME(tillt): Due to known issue when building libnvidia-container on improperly configured systems, we have
    # to rename and incorrectly named `libelf.so.1` and restart the build to finish. Unclear what the problem really
    # is - see https://github.com/NVIDIA/libnvidia-container/issues/116.
    - name: build libnvidia-container
      command: docker run -i -v /opt:/opt -v {{ libnvidia_dir.path }}/libnvidia-container:/tmp/libnvidia-container -w /tmp/libnvidia-container libnvidia-build:latest "/bin/sh" -c "make ; mv '/tmp/libnvidia-container/deps/src/elftoolchain-0.7.1/libelf/name libelf.so.1' /tmp/libnvidia-container/deps/src/elftoolchain-0.7.1/libelf/libelf.so.1 ; make"

    - name: copy nvidia-container-cli
      command: cp {{ libnvidia_dir.path }}/libnvidia-container/nvidia-container-cli /opt/bin

    - name: copy libnvidia-container
      command: cp {{ libnvidia_dir.path }}/libnvidia-container/libnvidia-container.so.{{ libnvidia_container_version }} /opt/lib/x86_64-linux-gnu

    - name: link libnvidia-container
      file:
        src: "/opt/lib/x86_64-linux-gnu/libnvidia-container.so.{{ libnvidia_container_version }}"
        dest: "/opt/lib/x86_64-linux-gnu/libnvidia-container.so.1"
        state: link

    - name: link libnvidia-container
      file:
        src: "/opt/lib/x86_64-linux-gnu/libnvidia-container.so.{{ libnvidia_container_version }}"
        dest: "/opt/lib/x86_64-linux-gnu/libnvidia-container.so"
        state: link

- name: run ldconfig
  changed_when: false
  command: ldconfig /opt/lib/x86_64-linux-gnu

# Activate services.
- name: enable nvidia-driver service
  changed_when: false
  service:
    name: nvidia-driver
    enabled: yes
    state: started
  notify:
    - reload systemd

- name: enable nvidia-persistenced service
  changed_when: false
  service:
    name: nvidia-persistenced
    enabled: yes
    state: started
  notify:
    - reload systemd
